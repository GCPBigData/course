{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNNs.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"DXCoZleX0He8","colab_type":"text"},"cell_type":"markdown","source":["\n","# Convolutional Neural Networks\n","\n","Today's class will be given with the following plan:\n","\n","\n","1.   Motivation\n","2.   What is a CNN?\n","3.   Further Intuition\n","3.   Preparing the data\n","4.   Building a CNN\n","5.   Training and evaluation\n","6.    Saving the model for reuse\n","\n","\n","\n","- https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050\n","\n","\n","## 1. Motivation\n","Convolutional Neural Networks are responsible for some of the most advanced advancements in Deep Learning.  Here's some examples of complex structures that can be extracted by using convolutional neural networks ([source](https://arxiv.org/pdf/1311.2901.pdf)):\n","![](https://cdn-images-1.medium.com/max/1400/1*TopKS7puyHfrmPohXseLAQ.png)\n","\n","\n","\n","## 2. What's a CNN?\n","Here's the traditional way that neural networks work:\n","\n","![1*vlgEOOMh9UcWkRsX7OniJQ.gif](https://cdn-images-1.medium.com/max/1600/1*vlgEOOMh9UcWkRsX7OniJQ.gif)\n","\n","When the input is images, there are two main problems:\n","- The information regarding close pixels is not preserved\n","- The number of inputs is very big\n","\n","In order to handle both problems, we can use Convolutional Neural Networks. The way it works can be summarized as follows:\n","\n","![](https://ahmedbesbes.com/images/GIF.gif)\n","\n","It's strategy is based on convolutional transformations. Convolution is like a 'filter' that's applied to an image in order to extract features. Mathematically, it is a matrix that is multiplied to the original image and then averaged. Here's an example of convolution being applied to a matrix:\n","\n","![](https://santexgroup.com/wp-content/uploads/2018/01/stride2.gif)\n","\n","Kernel is a matrix (usually 3X3) with values that defines the type of translation to be performed. Here are some examples of kernels:\n","![](https://santexgroup.com/wp-content/uploads/2018/01/Screen-Shot-2018-01-11-at-12.22.38-PM.png)\n","\n","So basically the operation is performed through all pixels around the image. They are multiplied by the kernel's value and then summed. Here's a visual example for edge detection:\n","\n","![](https://media.giphy.com/media/4VWs21TCbpO14haseT/giphy.gif)\n","\n","\n","You can further play [here](http://setosa.io/ev/image-kernels/). The previous example is for a black and white image which can be represented by a 2D matrix. But what about colorful images? In colorful images, we actually have 3 bi-dimentional matrices (RGB for example). Hence, the convolution, becomes 3D. Here's an example:\n","![](https://cdn-images-1.medium.com/max/800/1*_34EtrgYk6cQxlJ2br51HQ.gif)\n","\n","\n","Those convolutions are combined with a very simple strategy to reduce the matrices dimension called [maxpooling]():\n","\n","![](https://developers.google.com/machine-learning/practica/image-classification/images/maxpool_animation.gif)\n","\n","In short, maxpooling is going to declare a filter in which the maximum value is extracted for each grid. In the end, the idea is to reduce the image to an array indicating the presence or not of given features:\n","\n","![](https://cdn-images-1.medium.com/max/1200/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)\n","\n","This should be learn't by the computer itself without any inference from a human.\n","\n","https://www.quora.com/How-are-convolutional-filters-kernels-initialized-and-learned-in-a-convolutional-neural-network-CNN\n","\n","## 3. Further Intuition\n","Please refer to [this link](https://docs.google.com/spreadsheets/d/1cTjEggPifr9AfymmGEsA9eHlqXzUNu20_JszRR6j_lI/edit#gid=764381460) for a visual explanation using Google Sheets. \n","\n","\n","\n","\n"]},{"metadata":{"id":"v6OCC73yRnNv","colab_type":"text"},"cell_type":"markdown","source":["## 4. Practical Example: MNIST Dataset\n","### 4.1 Preparing the input data\n","- Import\n","- Have a look in the dataset"]},{"metadata":{"id":"PeFI3cQYRmbT","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.datasets import mnist\n","\n","# the data, split between train and test sets\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aTEMyoXzTNCK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"b410e2f5-debd-4345-c348-12e4ae217d0f","executionInfo":{"status":"ok","timestamp":1537799117865,"user_tz":-180,"elapsed":1263,"user":{"displayName":"Fernando Marcos Wittmann","photoUrl":"//lh5.googleusercontent.com/-IMybAdFO0xk/AAAAAAAAAAI/AAAAAAAAUuE/YzSiSBdfaIc/s50-c-k-no/photo.jpg","userId":"105799216615715799941"}}},"cell_type":"code","source":["print(\"Shape of the training set: {}\".format(X_train.shape))\n","print(\"Shape of the test set: {}\".format(X_test.shape))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape of the training set: (60000, 28, 28)\n","Shape of the test set: (10000, 28, 28)\n"],"name":"stdout"}]},{"metadata":{"id":"UJpUG_38US0Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"eb94ba63-54af-46ad-bdd8-f5653aa4a01d","executionInfo":{"status":"ok","timestamp":1537799134669,"user_tz":-180,"elapsed":766,"user":{"displayName":"Fernando Marcos Wittmann","photoUrl":"//lh5.googleusercontent.com/-IMybAdFO0xk/AAAAAAAAAAI/AAAAAAAAUuE/YzSiSBdfaIc/s50-c-k-no/photo.jpg","userId":"105799216615715799941"}}},"cell_type":"code","source":["y_train[IDX]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"5bVJdlEUUmZa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":362},"outputId":"96e47984-8e43-4225-8614-e8b11fd1e87a","executionInfo":{"status":"ok","timestamp":1537799174399,"user_tz":-180,"elapsed":845,"user":{"displayName":"Fernando Marcos Wittmann","photoUrl":"//lh5.googleusercontent.com/-IMybAdFO0xk/AAAAAAAAAAI/AAAAAAAAUuE/YzSiSBdfaIc/s50-c-k-no/photo.jpg","userId":"105799216615715799941"}}},"cell_type":"code","source":["# Now, let's visualize some digits from the training set:\n","import matplotlib.pyplot as plt\n","import numpy as np\n","for IDX in range(1):\n","  plt.imshow(X_train[IDX], cmap='gray')\n","  plt.title(\"Label: {}\".format(y_train[IDX]))\n","  plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7ff12bf9eef0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"9ENmPMuKVolZ","colab_type":"text"},"cell_type":"markdown","source":["Let's now normalize the data:"]},{"metadata":{"id":"KEln1_OSVsEE","colab_type":"code","colab":{}},"cell_type":"code","source":["# Normalize the data\n","X_train, X_test = X_train / 255.0, X_test / 255.0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-EuPGiCJ-lmB","colab_type":"text"},"cell_type":"markdown","source":["Normalisation is required so that all the inputs are at a comparable range.\n","Say there are two inputs to your ann, x1 and x2. x1 varies from to 0 to 0.5 and x2 varies from 0 to 1000. A change of x1 of 0.5  is 100 percent change where as a change of x2 by 0.5 is only a change of 0.05%. Hence normalization helps"]},{"metadata":{"id":"P2z_A6xiRvVg","colab_type":"text"},"cell_type":"markdown","source":["### 4.2 Training with a simple MLP first"]},{"metadata":{"id":"Mxa9gmq-4-PD","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"kml7BUpv4-_-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"outputId":"5de098b6-8133-46c5-b8af-744d32e14697","executionInfo":{"status":"error","timestamp":1537211428864,"user_tz":-180,"elapsed":606,"user":{"displayName":"Fernando Marcos Wittmann","photoUrl":"//lh5.googleusercontent.com/-IMybAdFO0xk/AAAAAAAAAAI/AAAAAAAAUuE/YzSiSBdfaIc/s50-c-k-no/photo.jpg","userId":"105799216615715799941"}}},"cell_type":"code","source":["from keras.models import Sequential\n","\n","from keras.layers import Input, Dense, Conv2D\n","\n","\n","##model building\n","model = Sequential()\n","#convolutional layer with rectified linear unit activation\n","model.add(Conv2D(32, kernel_size=(3, 3),\n","                 activation='relu',\n","                 input_shape=input_shape))\n","#32 convolution filters used each of size 3x3\n","#again\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","#64 convolution filters used each of size 3x3\n","#choose the best features via pooling\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","#randomly turn neurons on and off to improve convergence\n","model.add(Dropout(0.25))\n","#flatten since too many dimensions, we only want a classification output\n","model.add(Flatten())\n","#fully connected to get all relevant data\n","model.add(Dense(128, activation='relu'))\n","#one more dropout for convergence' sake :) \n","model.add(Dropout(0.5))\n","#output a softmax to squash the matrix into output probabilities\n","model.add(Dense(num_category, activation='softmax'))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-805e4ede5733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m model.add(Conv2D(32, kernel_size=(3, 3),\n\u001b[1;32m     10\u001b[0m                  \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                  input_shape=input_shape))\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#32 convolution filters used each of size 3x3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'input_shape' is not defined"]}]},{"metadata":{"id":"J3b9khPL4_Tl","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"ojnjfD1b4_hI","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"sxdmW7y34_uh","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"S5wZ60Qh4_8U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":745},"outputId":"8720577b-adf9-400f-b2ad-5cf092be5031","executionInfo":{"status":"error","timestamp":1537801435478,"user_tz":-180,"elapsed":2124,"user":{"displayName":"Fernando Marcos Wittmann","photoUrl":"//lh5.googleusercontent.com/-IMybAdFO0xk/AAAAAAAAAAI/AAAAAAAAUuE/YzSiSBdfaIc/s50-c-k-no/photo.jpg","userId":"105799216615715799941"}}},"cell_type":"code","source":["import tensorflow as tf\n","mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])\n","model.compile(optimizer='adam')#,\n","              #loss='sparse_categorical_crossentropy',\n","              #metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=5)\n","score = model.evaluate(x_test, y_test)\n","\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Output \"dense_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_1\" during training.\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-f755e3600025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m               \u001b[0;31m#metrics=['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    839\u001b[0m                      \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m                      \u001b[0mloss_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m                      target_tensors=target_tensors)\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# In graph mode, if we had just set inputs and targets as symbolic tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m           raise ValueError('The model cannot be compiled '\n\u001b[0m\u001b[1;32m    463\u001b[0m                            'because it has no loss to optimize.')\n\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The model cannot be compiled because it has no loss to optimize."]}]},{"metadata":{"id":"jgeCrlak5AMG","colab_type":"text"},"cell_type":"markdown","source":["### 4.3 Extending to CNNs"]},{"metadata":{"id":"aiBni2ma5AVe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"outputId":"384c7394-fc63-4f4c-981d-d275b1db42ec","executionInfo":{"status":"ok","timestamp":1537281017006,"user_tz":-180,"elapsed":99645,"user":{"displayName":"Fernando Marcos Wittmann","photoUrl":"//lh5.googleusercontent.com/-IMybAdFO0xk/AAAAAAAAAAI/AAAAAAAAUuE/YzSiSBdfaIc/s50-c-k-no/photo.jpg","userId":"105799216615715799941"}}},"cell_type":"code","source":["'''Trains a simple convnet on the MNIST dataset.\n","Gets to 99.25% test accuracy after 12 epochs\n","(there is still a lot of margin for parameter tuning).\n","16 seconds per epoch on a GRID K520 GPU.\n","'''\n","\n","from __future__ import print_function\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","\n","batch_size = 128\n","num_classes = 10\n","epochs = 12\n","\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=10)\n","\n","\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3),\n","                 activation='relu',\n","                 input_shape=input_shape))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_val, y_val))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["x_train shape: (60000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n","Train on 45000 samples, validate on 15000 samples\n","Epoch 1/12\n","45000/45000 [==============================] - 9s 190us/step - loss: 0.3186 - acc: 0.9023 - val_loss: 0.0840 - val_acc: 0.9741\n","Epoch 2/12\n","45000/45000 [==============================] - 8s 179us/step - loss: 0.1045 - acc: 0.9693 - val_loss: 0.0785 - val_acc: 0.9759\n","Epoch 3/12\n","45000/45000 [==============================] - 8s 178us/step - loss: 0.0761 - acc: 0.9772 - val_loss: 0.0477 - val_acc: 0.9859\n","Epoch 4/12\n","45000/45000 [==============================] - 8s 179us/step - loss: 0.0603 - acc: 0.9818 - val_loss: 0.0439 - val_acc: 0.9877\n","Epoch 5/12\n","45000/45000 [==============================] - 8s 180us/step - loss: 0.0524 - acc: 0.9838 - val_loss: 0.0403 - val_acc: 0.9875\n","Epoch 6/12\n","45000/45000 [==============================] - 8s 178us/step - loss: 0.0460 - acc: 0.9860 - val_loss: 0.0452 - val_acc: 0.9877\n","Epoch 7/12\n","45000/45000 [==============================] - 8s 177us/step - loss: 0.0431 - acc: 0.9872 - val_loss: 0.0376 - val_acc: 0.9890\n","Epoch 8/12\n","45000/45000 [==============================] - 8s 177us/step - loss: 0.0384 - acc: 0.9878 - val_loss: 0.0367 - val_acc: 0.9889\n","Epoch 9/12\n","45000/45000 [==============================] - 8s 177us/step - loss: 0.0355 - acc: 0.9886 - val_loss: 0.0342 - val_acc: 0.9894\n","Epoch 10/12\n","45000/45000 [==============================] - 8s 178us/step - loss: 0.0327 - acc: 0.9898 - val_loss: 0.0401 - val_acc: 0.9887\n","Epoch 11/12\n","45000/45000 [==============================] - 8s 178us/step - loss: 0.0290 - acc: 0.9910 - val_loss: 0.0346 - val_acc: 0.9907\n","Epoch 12/12\n","45000/45000 [==============================] - 8s 179us/step - loss: 0.0286 - acc: 0.9909 - val_loss: 0.0353 - val_acc: 0.9901\n","Test loss: 0.028247276519634396\n","Test accuracy: 0.9919\n"],"name":"stdout"}]},{"metadata":{"id":"avlDzJ_8B-GF","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9DTxOKchEJvU","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"regNnGxoEsoS","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}