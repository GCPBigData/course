{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Loading Image Datasets.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WittmannF/course/blob/master/day-3/Loading_Image_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obNbML1oLNJd",
        "colab_type": "text"
      },
      "source": [
        "# Loading and Preprocessing Images\n",
        "As an example we will be loading a dataset with images of cats and dogs using both `flow_from_dataframe` and `flow_from_directory` methods. First of all, let's clone the image dataset. The dataset contains 256 training images and 64 validation images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwpc5mb2LMjI",
        "colab_type": "code",
        "outputId": "c60a8bcb-de48-4b4e-db07-ad4bf8eadeae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/WittmannF/ImageDataGenerator-example.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ImageDataGenerator-example'...\n",
            "remote: Enumerating objects: 341, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/341)   \u001b[K\rremote: Counting objects:   1% (4/341)   \u001b[K\rremote: Counting objects:   2% (7/341)   \u001b[K\rremote: Counting objects:   3% (11/341)   \u001b[K\rremote: Counting objects:   4% (14/341)   \u001b[K\rremote: Counting objects:   5% (18/341)   \u001b[K\rremote: Counting objects:   6% (21/341)   \u001b[K\rremote: Counting objects:   7% (24/341)   \u001b[K\rremote: Counting objects:   8% (28/341)   \u001b[K\rremote: Counting objects:   9% (31/341)   \u001b[K\rremote: Counting objects:  10% (35/341)   \u001b[K\rremote: Counting objects:  11% (38/341)   \u001b[K\rremote: Counting objects:  12% (41/341)   \u001b[K\rremote: Counting objects:  13% (45/341)   \u001b[K\rremote: Counting objects:  14% (48/341)   \u001b[K\rremote: Counting objects:  15% (52/341)   \u001b[K\rremote: Counting objects:  16% (55/341)   \u001b[K\rremote: Counting objects:  17% (58/341)   \u001b[K\rremote: Counting objects:  18% (62/341)   \u001b[K\rremote: Counting objects:  19% (65/341)   \u001b[K\rremote: Counting objects:  20% (69/341)   \u001b[K\rremote: Counting objects:  21% (72/341)   \u001b[K\rremote: Counting objects:  22% (76/341)   \u001b[K\rremote: Counting objects:  23% (79/341)   \u001b[K\rremote: Counting objects:  24% (82/341)   \u001b[K\rremote: Counting objects:  25% (86/341)   \u001b[K\rremote: Counting objects:  26% (89/341)   \u001b[K\rremote: Counting objects:  27% (93/341)   \u001b[K\rremote: Counting objects:  28% (96/341)   \u001b[K\rremote: Counting objects:  29% (99/341)   \u001b[K\rremote: Counting objects:  30% (103/341)   \u001b[K\rremote: Counting objects:  31% (106/341)   \u001b[K\rremote: Counting objects:  32% (110/341)   \u001b[K\rremote: Counting objects:  33% (113/341)   \u001b[K\rremote: Counting objects:  34% (116/341)   \u001b[K\rremote: Counting objects:  35% (120/341)   \u001b[K\rremote: Counting objects:  36% (123/341)   \u001b[K\rremote: Counting objects:  37% (127/341)   \u001b[K\rremote: Counting objects:  38% (130/341)   \u001b[K\rremote: Counting objects:  39% (133/341)   \u001b[K\rremote: Counting objects:  40% (137/341)   \u001b[K\rremote: Counting objects:  41% (140/341)   \u001b[K\rremote: Counting objects:  42% (144/341)   \u001b[K\rremote: Counting objects:  43% (147/341)   \u001b[K\rremote: Counting objects:  44% (151/341)   \u001b[K\rremote: Counting objects:  45% (154/341)   \u001b[K\rremote: Counting objects:  46% (157/341)   \u001b[K\rremote: Counting objects:  47% (161/341)   \u001b[K\rremote: Counting objects:  48% (164/341)   \u001b[K\rremote: Counting objects:  49% (168/341)   \u001b[K\rremote: Counting objects:  50% (171/341)   \u001b[K\rremote: Counting objects:  51% (174/341)   \u001b[K\rremote: Counting objects:  52% (178/341)   \u001b[K\rremote: Counting objects:  53% (181/341)   \u001b[K\rremote: Counting objects:  54% (185/341)   \u001b[K\rremote: Counting objects:  55% (188/341)   \u001b[K\rremote: Counting objects:  56% (191/341)   \u001b[K\rremote: Counting objects:  57% (195/341)   \u001b[K\rremote: Counting objects:  58% (198/341)   \u001b[K\rremote: Counting objects:  59% (202/341)   \u001b[K\rremote: Counting objects:  60% (205/341)   \u001b[K\rremote: Counting objects:  61% (209/341)   \u001b[K\rremote: Counting objects:  62% (212/341)   \u001b[K\rremote: Counting objects:  63% (215/341)   \u001b[K\rremote: Counting objects:  64% (219/341)   \u001b[K\rremote: Counting objects:  65% (222/341)   \u001b[K\rremote: Counting objects:  66% (226/341)   \u001b[K\rremote: Counting objects:  67% (229/341)   \u001b[K\rremote: Counting objects:  68% (232/341)   \u001b[K\rremote: Counting objects:  69% (236/341)   \u001b[K\rremote: Counting objects:  70% (239/341)   \u001b[K\rremote: Counting objects:  71% (243/341)   \u001b[K\rremote: Counting objects:  72% (246/341)   \u001b[K\rremote: Counting objects:  73% (249/341)   \u001b[K\rremote: Counting objects:  74% (253/341)   \u001b[K\rremote: Counting objects:  75% (256/341)   \u001b[K\rremote: Counting objects:  76% (260/341)   \u001b[K\rremote: Counting objects:  77% (263/341)   \u001b[K\rremote: Counting objects:  78% (266/341)   \u001b[K\rremote: Counting objects:  79% (270/341)   \u001b[K\rremote: Counting objects:  80% (273/341)   \u001b[K\rremote: Counting objects:  81% (277/341)   \u001b[K\rremote: Counting objects:  82% (280/341)   \u001b[K\rremote: Counting objects:  83% (284/341)   \u001b[K\rremote: Counting objects:  84% (287/341)   \u001b[K\rremote: Counting objects:  85% (290/341)   \u001b[K\rremote: Counting objects:  86% (294/341)   \u001b[K\rremote: Counting objects:  87% (297/341)   \u001b[K\rremote: Counting objects:  88% (301/341)   \u001b[K\rremote: Counting objects:  89% (304/341)   \u001b[K\rremote: Counting objects:  90% (307/341)   \u001b[K\rremote: Counting objects:  91% (311/341)   \u001b[K\rremote: Counting objects:  92% (314/341)   \u001b[K\rremote: Counting objects:  93% (318/341)   \u001b[K\rremote: Counting objects:  94% (321/341)   \u001b[K\rremote: Counting objects:  95% (324/341)   \u001b[K\rremote: Counting objects:  96% (328/341)   \u001b[K\rremote: Counting objects:  97% (331/341)   \u001b[K\rremote: Counting objects:  98% (335/341)   \u001b[K\rremote: Counting objects:  99% (338/341)   \u001b[K\rremote: Counting objects: 100% (341/341)   \u001b[K\rremote: Counting objects: 100% (341/341), done.\u001b[K\n",
            "remote: Compressing objects: 100% (338/338), done.\u001b[K\n",
            "remote: Total 341 (delta 4), reused 336 (delta 2), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (341/341), 6.69 MiB | 12.68 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X2GRq5HL6Ap",
        "colab_type": "code",
        "outputId": "1008f0ba-57f3-43ea-d3e1-7e1da591883e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd ImageDataGenerator-example"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ImageDataGenerator-example\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIzRuD_zL-5A",
        "colab_type": "code",
        "outputId": "4e2dcdc5-5232-415c-90ee-3e523a0478ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mflow_from_dataframe\u001b[0m/  \u001b[01;34mflow_from_directory\u001b[0m/  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pedUOXRTQOUz",
        "colab_type": "text"
      },
      "source": [
        "## Defining Base Model\n",
        "Let's define the base model that we will be using for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CFw0M6qQOfd",
        "colab_type": "code",
        "outputId": "6eb1a355-5246-40b3-e7d6-a5d6498cc8d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# 2. Initialize base model\n",
        "base_model = VGG16(include_top=False, input_shape=(224,224,3))\n",
        "\n",
        "# 3. Freeze layers from the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable=False\n",
        "    \n",
        "# 4. Add Fully connected layer\n",
        "model = Sequential([base_model,\n",
        "                    Flatten(),\n",
        "                    Dense(1024, activation='relu'),\n",
        "                    Dense(2, activation='softmax')])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0720 15:20:46.913207 140322684524416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0720 15:20:46.953242 140322684524416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0720 15:20:46.961207 140322684524416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0720 15:20:47.007959 140322684524416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0720 15:20:49.448697 140322684524416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0720 15:20:49.449717 140322684524416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNir_KmnQhj7",
        "colab_type": "code",
        "outputId": "42776e02-30aa-4de7-93cc-76ec3fa9560f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(lr=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0720 15:21:01.632138 140322684524416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hRpkEn5QlJy",
        "colab_type": "text"
      },
      "source": [
        "Since we are using the loss `sparse_categorical_crossentropy`, it is required as output sparse integer numbers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixs2vjLoB3Mt",
        "colab_type": "text"
      },
      "source": [
        "## `flow_from_directory` example\n",
        "The method `flow_from_directory` is used when image files are contained one subdirectory per class, for example:\n",
        "\n",
        "![Screen Shot 2019-07-05 at 13 50 38](https://user-images.githubusercontent.com/5733246/60736066-1919a800-9f2c-11e9-9c93-f327178cc478.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5yqXp1TLPCD",
        "colab_type": "code",
        "outputId": "93316b0d-9e82-4c2b-acb2-c18eb58d67fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 1. Define Data Generators\n",
        "TRAIN_PATH = 'flow_from_directory/train'\n",
        "\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "train_gen = datagen.flow_from_directory(TRAIN_PATH, target_size=(224, 224), class_mode=\"sparse\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHkH2PiYB2fK",
        "colab_type": "code",
        "outputId": "ab3ac7dc-29d1-4e97-beb2-e3a185d286da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_gen.class_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cat': 0, 'dog': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMg_tCiWJhhv",
        "colab_type": "text"
      },
      "source": [
        "## `flow_from_dataframe` example\n",
        "When using the method `flow_from_dataframe`, we have to define a dataframe with two columns, one with the path of each image file and the other with the class in which each image belongs. For example:\n",
        "\n",
        "![Screen Shot 2019-07-05 at 14 24 54](https://user-images.githubusercontent.com/5733246/60737318-b5de4480-9f30-11e9-88ab-455015d5e131.png)\n",
        "\n",
        "In those cases, the class can be infered either from the filename or from an additional file with the class of each filename. For both cases, we have to map each filepath into their correct class. Here's an example inferring from the filename:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff048c71-a4c9-45ee-9504-b6253677aecf",
        "id": "rBEPm4hNRken",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd flow_from_dataframe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ImageDataGenerator-example/flow_from_dataframe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa5cSD-8MkjO",
        "colab_type": "text"
      },
      "source": [
        "Let's get all the filenames from the training path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr2-HgdFMpdz",
        "colab_type": "code",
        "outputId": "cb8832b2-2d57-47b1-e792-15a491016c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import glob\n",
        "train = glob.glob('train/*.jpg')\n",
        "\n",
        "print(train[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['train/cat.107.jpg', 'train/dog.126.jpg', 'train/cat.106.jpg', 'train/cat.56.jpg', 'train/cat.28.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xOoM2-sMpu_",
        "colab_type": "text"
      },
      "source": [
        "Now, let's create a dataframe with both filepaths and classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rAvV6XgNLO3",
        "colab_type": "code",
        "outputId": "30deac2d-2a4f-4a18-ffd5-ba5b96e9cf11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "import pandas as pd\n",
        "# Convert filepaths to a Pandas dataframe\n",
        "train_df = pd.DataFrame({'filename': train})\n",
        "\n",
        "# Add new column with the label of each file\n",
        "train_df['class'] = train_df['filename'].apply(lambda x: 'cat' if 'cat.' in x else 'dog')\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train/cat.107.jpg</td>\n",
              "      <td>cat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train/dog.126.jpg</td>\n",
              "      <td>dog</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train/cat.106.jpg</td>\n",
              "      <td>cat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train/cat.56.jpg</td>\n",
              "      <td>cat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train/cat.28.jpg</td>\n",
              "      <td>cat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            filename class\n",
              "0  train/cat.107.jpg   cat\n",
              "1  train/dog.126.jpg   dog\n",
              "2  train/cat.106.jpg   cat\n",
              "3   train/cat.56.jpg   cat\n",
              "4   train/cat.28.jpg   cat"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_PUv8wZNXmn",
        "colab_type": "text"
      },
      "source": [
        "By default, the columns names should be **filename** and **class**, if not, they have to be specified. Next, we can simply use the method `flow_from_dataframe`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjwZaIEnNsA_",
        "colab_type": "code",
        "outputId": "99e33b27-b704-4593-fead-5832dcb1b2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 1. Define Data Generators\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "train_gen = datagen.flow_from_dataframe(train_df, target_size=(224, 224), \n",
        "                                        batch_size=32,\n",
        "                                        class_mode=\"sparse\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 256 validated image filenames belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyGq-Q-NqHD",
        "colab_type": "text"
      },
      "source": [
        "## Moving foward\n",
        "In both scenarios, we will have to use the model's method `fit_generator`, here's a minimal example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeAE5A9qPMO8",
        "colab_type": "code",
        "outputId": "6b20b3eb-5897-46b6-c070-ff1a6e9e32b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "number_of_batches = train_gen.n//train_gen.batch_size\n",
        "model.fit_generator(train_gen, number_of_batches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0720 15:24:36.793971 140322684524416 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "8/8 [==============================] - 8s 1s/step - loss: 3.5436 - acc: 0.7266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f110eadd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2zJjyEMrU3u",
        "colab_type": "code",
        "outputId": "e16ce8e7-1687-4f4d-b3fd-1039141b968e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_gen.n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_dZd6QnrWpX",
        "colab_type": "code",
        "outputId": "b8c21157-21b5-4546-d868-086752a6a382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_gen.batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neUp-wgsSAF0",
        "colab_type": "text"
      },
      "source": [
        "It is also a good idea to add a validation folder in order to evaluate the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAuUJYwrR_PS",
        "colab_type": "code",
        "outputId": "6ef1fd61-ed21-47a6-ef0d-d0ccb541a0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "valid = glob.glob('valid/*.jpg')\n",
        "\n",
        "valid_df = pd.DataFrame({'filename': valid})\n",
        "valid_df['class'] = valid_df['filename'].apply(lambda x: 'cat' if 'cat.' in x else 'dog')\n",
        "\n",
        "valid_gen = datagen.flow_from_dataframe(valid_df, target_size=(224, 224), class_mode=\"sparse\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 64 validated image filenames belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwHN-7pCS8q2",
        "colab_type": "code",
        "outputId": "f356acad-d182-44cc-b89a-dccdd0d0db99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "model.fit_generator(train_gen, number_of_batches, validation_data=valid_gen, validation_steps=valid_gen.n//valid_gen.batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "8/8 [==============================] - 2s 231ms/step - loss: 2.2487 - acc: 0.8477 - val_loss: 1.5111 - val_acc: 0.9062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9ec67a68d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPone6E56AIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}